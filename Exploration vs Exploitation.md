### Exploration vs Exploitation

[[Reinforcement Learning]]

A dilema in RL is whether an agent should exploit its existing knowledge or explore in the hopes of finding actions that generate increased reward.  This is espicially usefull if the agent finds itself stuck in a local minima.

One common attempt to balance the Exploration vs Exploitation tradeoff is the eplison-greedy strategy.  However, this strategy is as likely to discover good or bad actions.

One solution to mitigate this is to start with a high value for eplison, so start with the majority of exploratory actions.  As the agent learns to navigate the state space, reduce the value for eplison and therefore increase the maximisation of existing knowledge.

An alternative to epslion greedy is the Softmax Action Selection function.  Here, the weight of each action is mapped to a probability.  This probability value is used in selection of the next action taken.  

A "temperature" parameter allows the selection policy to be tuned. Temperatre values close to 1 correspond to pure exploitation (a "greedy" policy, where the highest-weighted action is always chosen).  temperature values close to 0 map to pure exploration (where each action has an equal probability of being chosen).